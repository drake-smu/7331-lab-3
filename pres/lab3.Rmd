---
title: "Association Rules"
author: "Che Cobb, Andy Heroy, Carson Drake, David Josephs"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: TRUE
    toc_float:
      collapsed: true
      smooth_scroll: false
    theme: paper
    df_print: paged
    keep_md: TRUE

---

```{r setup, echo = F}
if (knitr::is_latex_output()) {
	knitr::opts_chunk$set(dev = "tikz")
}
knitr::opts_knit$set(cache =T, autodep = T)
knitr::opts_chunk$set(warning = F)
knitr::opts_chunk$set(message = F)
knitr::opts_chunk$set(tidy = T, tidy.opts = list(comment = F))
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(comment = '#>')
knitr::opts_chunk$set(fig.path = 'fig/')
library(knitr)
```

```{r, echo = F}
read_chunk('../analysis/Lab3Start.R')
```
# Business Understanding

* [10 points] Describe the purpose of the data set you selected (i.e., why was this data bcollected in the first place?). How will you measure the effectiveness of a good
algorithm? Why does your chosen validation method make sense for this specific
dataset and the stakeholders needs?

We chose this dataset from the UCI's machine learning repository for its categorical
predictive attributes.  It contains 1994 Census data pulled from the US Census
database.  The prediction task we've set forth is to predict if a persons
salary range is >50k in a 1994, based on the various categorical/numerical
attributes in the census database. The link to the data source is below:

https://archive.ics.uci.edu/ml/datasets/census+income


# Data Understanding
* [10 points] Describe the meaning and type of data (scale, values, etc.) for each
attribute in the data file. Verify data quality: Are there missing values? Duplicate data? Outliers? Are those mistakes? How do you deal with these problems?
* [10 points] Visualize the any important attributes appropriately. Important: Provide an interpretation for any charts or graphs.

# Setup

First, we need to load in the necessary libraries:

```{r libs}
```

Then we need to load our dataset, as well as examine it a bit

```{r datadef}
```

Next lets get rid of some columns we are not going to use/that has some issues, and then lets break down our numeric variables into factors using `cut`

```{r cleanup}
```

Next, we convert the data to an object with class `transactions`, after viewing it again

```{r transact}
```

# Modeling and Evaluation

* Different tasks will require different evaluation methods. Be as thorough as possible when analyzing the data you have chosen and use visualizations of the results to explain the performance and expected outcomes whenever possible. Guide the reader through your analysis with plenty of discussion of the results.

Before we begin with our analysis, lets check out the rule frequencies within the dataset. We are looking for rules with `support >= .2`

```{r rulefreq}
```

# Rule mining

Next, lets mine some rules with the apriori algorithm, and then clean up redundant rules. We are still sorting out what to set the minsupp and minconf to.

```{r rulemine}
```

## Rule quality and inspection

Next, let us inspect the rules, and examine their quality

```{r quality}
```

### Plots

First lets view a scatterplot of our rules

```{r scatterplot}
```

Next lets look at  a balloon plot

```{r baloonplot}
```

Parallel plot

```{r plplot}
```

Two key plot

```{r kplot}
```

grouped plot

```{r gplot}
```

# alternate rule mining

```{r redux}
```

## Inspection

```{r inspec}
```

## Plotting

```{r plot2}
```

```{r bplo2}
```


```{r kplo2}
```

```{r gplo2}
```
