---
title: "Association Rules"
author: "Che Cobb, Andy Heroy, Carson Drake, David Josephs"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: TRUE
    toc_float:
      collapsed: true
      smooth_scroll: false
    theme: paper
    df_print: paged
    keep_md: TRUE

---

```{r setup, echo = F}
if (knitr::is_latex_output()) {
	knitr::opts_chunk$set(dev = "tikz")
}
knitr::opts_knit$set(cache =T, autodep = T)
knitr::opts_chunk$set(warning = F)
knitr::opts_chunk$set(message = F)
knitr::opts_chunk$set(tidy = T, tidy.opts = list(comment = F))
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(comment = '#>')
knitr::opts_chunk$set(fig.path = 'fig/')
library(knitr)
```

```{r, echo = F}
read_chunk('../analysis/Lab3Start.R')
```
# Business Understanding

* [10 points] Describe the purpose of the data set you selected (i.e., why was this data bcollected in the first place?). How will you measure the effectiveness of a good
algorithm? Why does your chosen validation method make sense for this specific
dataset and the stakeholders needs?

We chose this dataset from the UCI's machine learning repository for its categorical
predictive attributes.  It contains 1994 Census data pulled from the US Census
database.  The prediction task we've set forth is to predict if a persons
salary range is >50k in a 1994, based on the various categorical/numerical
attributes in the census database. The link to the data source is below:

https://archive.ics.uci.edu/ml/datasets/census+income


# Data Understanding
* [10 points] Describe the meaning and type of data (scale, values, etc.) for each
attribute in the data file. Verify data quality: Are there missing values? Duplicate data? Outliers? Are those mistakes? How do you deal with these problems?
* [10 points] Visualize the any important attributes appropriately. Important: Provide an interpretation for any charts or graphs.

Here we will discuss each attribute and give some description about its ranges.
 
 
#### Categorical Attributes
  * workclass - Which business sector do they work in?
  * education - What level of education received?
  * marital_status - What is their marriage history
  * occupation - What do they do for a living
  * relationship - Family member relation
  * race - What is the subjects race
  * gender - What is the subjects gender
  * native_country - Where is the subject originally from
  * income_bracket - Do they make over or under 50k/year
 
#### Continuous Attributes
  * age - How old is the subject?
  * fnlwgt - Sampling weight of observation
  * education_num - numerical encoding of education variable
  * capital_gain - income from investment sources, separate from wages/salary
  * capital_loss - losses from investment sources, separate from wages/salary
  * hours_per_week - How many hours a week did they work?
  
# Setup

First, we need to load in the necessary libraries:

```{r libs}
```

Then we need to load our dataset, as well as examine it a bit

```{r datadef}
```

Next lets get rid of some columns we are not going to use/that has some issues, and then lets break down our numeric variables into factors using `cut`

```{r cleanup}
```

Next, we convert the data to an object with class `transactions`, after viewing it again

```{r transact}
```

# Modeling and Evaluation

* Different tasks will require different evaluation methods. Be as thorough as possible when analyzing the data you have chosen and use visualizations of the results to explain the performance and expected outcomes whenever possible. Guide the reader through your analysis with plenty of discussion of the results. For this task, we chose Option B: Association Rule Mining.

Option B: Association Rule Mining
• Create frequent itemsets and association rules.
• Use tables/visualization to discuss the found results.
• Use several measure for evaluating how interesting different rules are.
• Describe your results. What findings are the most compelling and why?

Before we begin with our analysis, lets check out the rule frequencies within the dataset. We are looking for rules with `support >= .2`

```{r rulefreq}
```

# Rule mining

Next, lets mine some rules with the apriori algorithm, and then clean up redundant rules. We are still sorting out what to set the minsupp and minconf to.

```{r rulemine}
```

## Rule quality and inspection

Next, let us inspect the rules, and examine their quality

```{r quality}
```

### Plots

First lets view a scatterplot of our rules

```{r scatterplot}
```

Next lets look at  a balloon plot

```{r baloonplot}
```

Parallel plot

```{r plplot}
```

Two key plot

```{r kplot}
```

grouped plot

```{r gplot}
```

# alternate rule mining

```{r redux}
```

## Inspection

```{r inspec}
```

## Plotting

```{r plot2}
```

```{r bplo2}
```


```{r kplo2}
```

```{r gplo2}
```
