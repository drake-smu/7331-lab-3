---
title: "Lab 3: Association Rules"
author: "Che Cobb, Andy Heroy, Carson Drake, David Josephs"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: TRUE
    toc_float:
      collapsed: true
      smooth_scroll: false
    theme: paper
    df_print: paged
    keep_md: TRUE

---

```{r setup, echo = F}

if (knitr::is_latex_output()) {
	knitr::opts_chunk$set(dev = "tikz")
}
knitr::opts_chunk$set(warning = F)
# knitr::opts_knit$set(root.dir = "..")
knitr::opts_chunk$set(message = F)
knitr::opts_chunk$set(tidy = T, tidy.opts = list(comment = F))
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(comment = '#>')
knitr::opts_chunk$set(fig.path = 'fig/')
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())

```


# Project Overview

Please we need to discuss project motivation and our experiment, just as Rida asked

Then we can begin our analysis

# Project Setup


# Business Understanding

* [10 points] Describe the purpose of the data set you selected (i.e., why was
this data bcollected in the first place?). How will you measure the
effectiveness of a good algorithm? Why does your chosen validation method make
sense for this specific dataset and the stakeholders needs?

We chose this dataset from the UCI's machine learning repository for its categorical predictive attributes.  It contains 1994 Census data pulled from the US Census database.  The prediction task we've set forth is to predict if a person salary range is >50k in a 1994, based on the various categorical/numerical attributes in the census database. The link to the data source is below:

https://archive.ics.uci.edu/ml/datasets/census+income

The effectiveness of our algorithm will be determined by support, confidence and lift.  As these are the metrics that describe how strong a relationship between each element is with the other elements within each transaction.  Currently, there are no methods for cross validation of association rules, although there are some hard working individuals out there that are attempting to create such a tool.

- TODO - ADD DEFINITION FOR LIFT/CONFIDENCE/SUPPORT HERE


# Data Understanding
* [10 points] Describe the meaning and type of data (scale, values, etc.) for each
attribute in the data file. Verify data quality: Are there missing values? Duplicate data? Outliers? Are those mistakes? How do you deal with these problems?
* [10 points] Visualize the any important attributes appropriately. Important: Provide an interpretation for any charts or graphs.

Here we will discuss each attribute and give some description about its ranges.
 
 
#### Categorical Attributes
  * workclass - Which business sector do they work in?
  * education - What level of education received?
  * marital_status - What is their marriage history
  * occupation - What do they do for a living
  * relationship - Family member relation
  * race - What is the subjects race
  * gender - What is the subjects gender
  * native_country - Where is the subject originally from
  * income_bracket - Do they make over or under 50k/year
 
#### Continuous Attributes
  * age - How old is the subject?
  * fnlwgt - Sampling weight of observation
  * education_num - numerical encoding of education variable
  * capital_gain - income from investment sources, separate from wages/salary
  * capital_loss - losses from investment sources, separate from wages/salary
  * hours_per_week - How many hours a week did they work?
  
# Setup


First, lets go ahead and load up necessary libraries:

```{r libraries,echo=F, tidy=F, results='hold', comment=''}
source("scripts/load-packages.R",echo=F,print.eval = T, 
       skip.echo = 7,
       spaced = F,prompt.echo='')

writeLines(c("Loaded Packages:",
             strwrap(
               toString(LabPackages), 
               width = 60,
               indent = 3,
               exdent = 3)
             ))
```


```{r, echo = F}
# read in chunks here
read_chunk('analysis/preprocessing.R')
read_chunk('analysis/Lab3Start.R')
```

Next, lets import our dataset

```{r dataimport}
```

## Data Quality Inspection

The first thing we must do is check and see if there are any NAs in our dataset, just to make sure to not mess up our analysis.

```{r dataqual}
```


## Data Cleaning

Looks like we are doing ok here. The next issue we have in the dataset, is
because of the way the csv was stored, some of the levels in our factors include
leading and trailing whitespace. This is highly undesirable, so we must clean it
up:

```{r factorclean}
```

Next, we need to reencode our data as factors. First, lets encode the education levels into factors with larger groups (for example 1st-12th grade should be no diploma, not a bunch of levels).

```{r edubin}
```

Then the the income brackets:

```{r incbrack}
```

Next, lets change the `?` levels to something more useful:

```{r questionmark}
lapply(data[GetFactors(data)], FixLevels)
```


Next, lets remove the fnlwgt, education number, and capital gain and loss columns, as they are unneeded. We also need to rename some columns to be easier for us, and use the `cut` function to factorize our numeric variables

```{r cleanup}
```

Lets see the results:

```{r preprocessres}
```

# EDA

We'd also like to get a quick feel for the dataset through some visulizations.

```{r EDAGraph1}
```

Our first plot shows us a quick histogram of age groups and their income group.  Not surprisingly, the majority of the "large" income group is in middleaged and senior groups supporting the common knowledge that as you get older, you should be making more money. 

```{r EDAGraph2}
```

Our next plot is a stacked bar chart of income group by density of education.  We see here that higher education (Bachelors, Masters, Doctorate) yields a higher proportions of the "large" income bracket.  Professional school also about equal with Docotorate suggesting specilaized trades and higher education are the best bet for making over 50k a year. 

```{r EDAGraph3}
```

Using the similar plot style as above, we'll now look at income group by density of Marital Status.  Which too no surprise to us researchers, married couples have a distinct advantage over thoes that are unmarried.  In fact the next highest categories are Widowed and Divorced.  Which means, if you want to make more money in life, you should at least give marriage a try. 

```{r EDAGraph4}
```

Next, we'll compare income groups by density of Occupation.  This revealed a few obvious, but also a few interesting results.  Exec-managerial and Prof-specialty were the highest categories to no one's surprise.  Protective-serv, transport-moving and tech-support also show good density for making over 50k.  Some of the lowest categories thought were Armed-Forces, and house-cleaning/services.  

```{r EDAGraph6}
```

Lastly, we combine education and ocupation to see if we can notice any particular relationships between the two groups.  What kind of educations to the people with large incomes have?  Exec-managerial show a large amount of bachelor's and masters degree's, which means, more school definitely helps in that occupation.  Prof-specialty shows the best range of educational sectors respresented but we found it interesting that prof-school wasn't a larger representation here.  This is probably due to prof-specialty being a wider category than some in that you can be a professional in alot of different occupational sectors.

Armed-forces have an extremely low amount of education information which also makes sense as most are recruited right out of high school.  We would have expected to see a higher amount of Diploma's given out in this group, but perhaps the information is just missing from this dataset. 

Finally, we can set up our dataset as a transactional dataset to be in the proper data format for the Apriori algorithm:  

```{r transact}

```


# Modeling and Evaluation

* Different tasks will require different evaluation methods. Be as thorough as possible when analyzing the data you have chosen and use visualizations of the results to explain the performance and expected outcomes whenever possible. Guide the reader through your analysis with plenty of discussion of the results. For this task, we chose Option B: Association Rule Mining.

Option B: Association Rule Mining
• Create frequent itemsets and association rules.
• Use tables/visualization to discuss the found results.
• Use several measure for evaluating how interesting different rules are.
• Describe your results. What findings are the most compelling and why?

Before we begin with our analysis, lets check out the rule frequencies within the dataset. We are looking for rules with `support >= .2`

```{r rulefreq}
```


# Rule mining

Next, lets mine some rules with the apriori algorithm, and then clean up redundant rules. We are still sorting out what to set the minsupp and minconf to.

```{r rulemine}
```

## Rule quality and inspection

Next, let us inspect the rules, and examine their quality

```{r quality}
```

## Plots

First lets view a scatterplot of our rules

```{r scatterplot, fig.cap="Scatter Plot"}
```

Next lets look at a balloon plot

```{r baloonplot, fig.cap="Balloon Plot"}
```

The balloon plot is able to filter and zoom, which gives the ability to look through each income group and see what rules are feeding the analysis.  Just by seperating out the large income bracket, we begin to see that married-civ-spouce has one of the highest lift scores.  The next two highest were white and Male.  Which follows suit with our above research as well. 

In the small income bracket, we see rules such as (Never-married, fulltime), (female, private), (Private, never-married).  This would begins to tell us that if you're female, never-married and work full time, then you're chances are higher for making under 50k.  

```{r plplot, fig.cap="Parallel Plot"}
```


```{r kplot, fig.cap="Two Key Plot"}
```


```{r gplot, fig.cap="Grouped Plot"}
```

# Alternate Rule Mining

After our first attempt at mining for rules, we're going to relax our support to 0.1 and increase our confidence to 98% see what we get.  Aftewards, we'll target small and large incomes individually in order to see if we can isolate the itemsets that are going to be of highest influence. 

```{r redux}
```

We can see that this time around we've generated 36 rule's.  We'll put them through the same inspection and plots as previously to see if any other rules stand out or if we're seeing more of the same.  

## Inspection

```{r inspec}
```

## Plotting

```{r plot2}
```

```{r bplo2}
```


```{r kplo2}
```

```{r gplo2}
```

# Targeting Small and Large Income
#### Small Income
Now that we've scanned a couple of different variations of apriori and generated rules accordingly.  Lets split up the targeting in the rhs section to only target one of the income brackets at a time.  First we'll start with low income.  

```{r small_income_rules}
```

```{r small_inspec}
```

It looks like we've generated 25 rules for the low income bracket.  Lets take a look at the scatter plot and find our releveant itemsets/rules 

```{r small_plotscat}
```

We will discuss the results from small income targeting in the conclusion section.

#### Large Income
Our next step is to now target the large income bracket and see what rules shake out

```{r large_income_rules}
```

```{r large_inspec}
```

It looks like we've generated 25 rules for the low income bracket.  Lets take a look at the scatter plot and find our releveant itemsets/rules 

```{r large_plotscat}
```

We will discuss the results from large income targeting in the conclusion section.

# Conclusion
### Small Income

Interesting large Income rules:

* Not being married is associated with a low income. Out of our 25 best rules, 16 of them involved not being married, and our five highest lift rules all involved never married. This is clearly not a coincidence, however what this means and the cause of this is not so clear. We propose a few hypotheses about this:
  * Married people, motivated by love, work harder and make more money

  * Shallow society deems people without money harder to marry

* People who are married got married because they are more social, and thus they are given more raises, perform better in interviews, etc. In contrast, people who were not sociable enough to meet someone may not perform as well in interviews etc., giving them fewer opportunities to get a job

* Having just a diploma from high school does not get you out of the lower income bracket. This is unsurprising, as today you need in general a minimum of a college education to start making good money (and even that is difficult). 5 out of our 25 rules involved highest education level = high school diploma

* 12 out of 25 rules involved being young. This makes perfect sense, as entry level jobs which young people get do not pay very well.

* 8 out of 25 rules involved being in the private sector. This makes sense, as the private sector contains many low wage jobs. 

* 4 out of 25 rules involved having a child. If you have a child, you ***should*** be spending more time with them, and maybe focusing a little less on your career. Therefore, you will probably earn less. This pattern makes sense.

* The rules for our small income target have a support of 12% and confidence of 95%. This means that (never-married, Diploma, Young, Private, and child) are observed in 12% of the dataset. We therefore believe that 95% of the time any of the attributes (never-married, Diploma, Young, Private, and child) are present, it will result in a small income.

### Large Income

Interesting large Income rules:

*	Thankfully, opposite to the small income analysis, large income targeting shows that being married greatly influences your income as it shows up in 9 of the 15 rules present.  Meaning again that by working as a team, you get more out of life.  So go find yourself a husband/wife.  

*	The next highest frequent itemset is hard-working that shows up in 8 of the 15 rules.  Which makes perfect sense that the more time you put into a job, the likelier you are for promotion and advancement within an organization.  

*	Education = Bachelors , native_country = United-States, relationship = husband are present in 5 out of the 15 rules.  

* The rules for our large income target have a support of 05% and confidence of 55%. This means that (married, hard-working, Bachelors, United-States, Husband) are observed in 5% of the dataset. We therefore believe that 55% of the time any of the attributes (married, hard-working, Bachelors, United-States, Husband) are present, it will result in a large income.


# Deployment
Be critical of your performance and tell the reader how you current model might
be usable by other parties. Did you achieve your goals? If not, can you reign in
the utility of your modeling?

• How useful is your model for interested parties (i.e., the companies or organizations that might want to use it)?
• How would your deploy your model for interested parties?
• What other data should be collected?
• How often would the model need to be updated, etc.?

# Exceptional Work
